{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOmhKGVGA+6rmK8kCGSqP3O",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ginograf/DSF/blob/main/Midterm_project_questions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Group 3: Gino Graf, Mathias Kissling**"
      ],
      "metadata": {
        "id": "WK9YQa-18FJn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Task 2: Answers of background questions**"
      ],
      "metadata": {
        "id": "62__FXpPiYh1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Which packages are available for ML? Describe the pros and cons and document the \n",
        "availability.\n"
      ],
      "metadata": {
        "id": "mV1S0EuGzJdz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Many useful packages are available for machine learning. In python the most common packages are:\n",
        "\n",
        "TensorFlow: provides tools for building and training deep neural networks. For windows user the availability is limited while for Linux users there are a wide range of features available.\n",
        "\n",
        "pros:\n",
        "- good data visualization\n",
        "- compatible with many programming languages such as python, JavaScript etc.\n",
        "- not limited to one specific device\n",
        "\n",
        "cons:\n",
        "- no windows support\n",
        "- quite slow\n",
        "- has only support for GPU\n",
        "\n",
        "Pytorch: It is a deep learning framework introduced by facebook. It is freely available. To install pytorch, the two package managers pip and conda should be installed on the system.\n",
        "\n",
        "pros:\n",
        "- user-friendly\n",
        "- has support for GPU and CPU\n",
        "- has computational graph support at runtime\n",
        "- supports cloud platforms\n",
        "\n",
        "cons:\n",
        "- developer community is small compared to other frameworks\n",
        "- has fewer users compared to other frameworks\n",
        "- Absence of monitoring and visualization tools\n",
        "\n",
        "Scikit-learn: It is focused on modeling the data. It is freely available and no licence is required.\n",
        "\n",
        "pros:\n",
        "- user-friendly\n",
        "- properly documented\n",
        "- versatile used\n",
        "\n",
        "cons:\n",
        "- not suitable for in depth-learning"
      ],
      "metadata": {
        "id": "Xxfmvkdp1kdg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What is Chembl? How do you access it?\n"
      ],
      "metadata": {
        "id": "YHKWRMAAzXxZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chembl is one of the largest databases for bioactive molecules with druglike-properties. It is accessible through the chembl website or its data can be downloaded from the ChEMBL FTP site."
      ],
      "metadata": {
        "id": "Ih4xPOlT037C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What is machine learning, and how does it differ from traditional programming?"
      ],
      "metadata": {
        "id": "ezBDFnNvzbW1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Machine learning is a part of artificial intelligence, that learns or improves patterns or relationships based on the data it uses without having been explicitly programmed. In traditional programming, the developer gives very specific instructions and dictates what the computer should do to achieve a specific output. With machine learning, the algorithm finds the rules for himself, i.e. the software is written independently in order to achieve the desired output from the data by using statistical methods and development programs that can be adjusted depending on external input."
      ],
      "metadata": {
        "id": "XhZOE1MI1j4G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What are the key concepts and techniques in machine learning?"
      ],
      "metadata": {
        "id": "xrTFJf6VzePw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are supervised learning, unsupervised learning and reinforcement learning which are described in the next question. Deep learning is a technique which uses neural networks to learn representations of data. Other key concepts are optimization (the process to find optimal parameters), evaluation, regularization and much more."
      ],
      "metadata": {
        "id": "ZjOrtrEk1lh2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. What are the different types of machine learning algorithms?"
      ],
      "metadata": {
        "id": "gZr9bD6GzgzQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Supervised learning: examples of inputs and desired outputs are used for learning. Example: The market value of a house (output) can be predicted based on living space, year of construction, etc. (input).\n",
        "- Unsupervised learning: The algorithm must independently recognize patterns in the input and analyze them, from which predictions can be made \n",
        "- Semi-supervised learning: Uses labelled and unlabelled data. With a small amount of labelled data the algorithm can understand the data and label unlabelled datasets. \n",
        "- Reinforcement learning: Algorithm interacts with a dynamic environment. Receives feedback while solving the task and based on that it learns to determine the right behavior in the given context."
      ],
      "metadata": {
        "id": "9RE5nNMa1mp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. What are the common applications of machine learning?"
      ],
      "metadata": {
        "id": "kHZZxTsCzg-2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Common applications of machine learning are for example:\n",
        "- Image recognition: Serves to detect and identify specific object or person or place in a digital image\n",
        "- Traffic prediction\n",
        "- Virtual Assistant, for example Siri or Alexa\n",
        "- Product recommendations\n",
        "- Email Spam filtering \n",
        "- Medical Diagnosis\n",
        "- Stock market trading\n",
        "- ..."
      ],
      "metadata": {
        "id": "ZNf9SNi71n3P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. How do you evaluate the performance of a machine learning model?\n"
      ],
      "metadata": {
        "id": "LGCeTjL1znP1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For example, the accuracy can be calculated, which gives the percentage of correctly performance of the model. Alternatively, the mean squared error or root mean squared error can be used, which gives the difference between the predicted and actual values in regression model. There are several other methods to evaluate the performance of a ML model and depending on the case, one or the other is more appropriate."
      ],
      "metadata": {
        "id": "M2iu47P91ool"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. How do you prepare data for use in a machine learning model?"
      ],
      "metadata": {
        "id": "3VKORra4zpc-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First the raw data has to be converted into another format like \"csv\", which is compatible to analyze the data. This is called data transformation. Then the data should be analyzed to find outliers, missing values etc. By cleaning the data, missing values can be added and outliers removed. The next step is then feature selection, where the most important features that are relevant to the problem being solved are identified. Then the data can be split into training, validation and testing sets to improve the perfomance of the model. Finally, increasing the size of the dataset can improve the robustness of the model. \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zMRSiYg71pe8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. What are some common challenges in machine learning, and how can they be \n",
        "addressed?"
      ],
      "metadata": {
        "id": "-H7U18rSzt4e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some main challenges and problems in machine learning are:\n",
        "- Bad data quality. Bad or incomplete data are a big problem for machine learning. Taking time to evaluate data to get to get data with good data can help.\n",
        "-  Lack of training data: The program needs examples and informations - training data- to learn and make good predictions.\n",
        "- Underfitting: The program or algorithm is too simple for analyzing the complex data. So the program must be improved to overcome this problem.\n",
        "- Overvitting: The opposite of underfitting: The ML model is too complicated for the data or it is trained to well on the training data that doesn't recognize patterns in new data.\n"
      ],
      "metadata": {
        "id": "yhQChxxQ1qSJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10.What are some resources and tools available to help you learn and practice machine \n",
        "learning?"
      ],
      "metadata": {
        "id": "B4cwS3Glzwm3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- GitHub, where developers share their code and where many repositories about machine learning are available.\n",
        "- Machine learning libraries like Tensor-Flow, Theano, Keras etc.\n",
        "- Online communities like Stack Overflow where questions about machine learning can be asked and you can connect with other users.\n",
        "- Online courses about machine learning"
      ],
      "metadata": {
        "id": "sZGe9KElzzmF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 3: Tutorial questions**"
      ],
      "metadata": {
        "id": "dOEsyrIcjzXW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Important steps of the tutorial are:\n",
        "\n",
        "First the data is modified, which means that the necessary data is filtered and selected from the actual dataframe and split into a training and test set. After modifying the data, the model can be defined and then compiled. Then the learning process takes place with the training set. In the training phase, a a loss function an optimizer are defined to get the prediction's error and to minimize it. Then the model is evaluated by the difference of the predicted and real values of the test data with the evaluate() method. Finally predictions are made based on the training model."
      ],
      "metadata": {
        "id": "JDqX5ygeiZcv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is in the training set, how big is it?"
      ],
      "metadata": {
        "id": "l9U4x_lk1YqU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The training set contains real data, which is used to train the neural network and to reduce the difference between the predicted value and the real value. In this case, the dataset is separated into training set (70%) and test set (30%)."
      ],
      "metadata": {
        "id": "5uvnR0pQ3_y7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What modifications do you need to do to the data set to perform the tutorial?"
      ],
      "metadata": {
        "id": "HID9rvDT4AOL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The required informations must be filtered and selected from the actual dataset and the data has to be split into training and test set. If the dataset contains missing values, then they should be removed. "
      ],
      "metadata": {
        "id": "e3JR76uL4DxE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What is a test set? Any other types of set?"
      ],
      "metadata": {
        "id": "do79sreg4EdC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The test set is applied after the training set and is used to evaluate the predictions of the model. Another set is the training set or validation set. "
      ],
      "metadata": {
        "id": "qSdD1F5C4Ehy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Before starting describe with 1-2 sentences, in your own words, what is done in each \n",
        "of the cells."
      ],
      "metadata": {
        "id": "Pw1IlsO44IJz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The descriptions are always at the end of the cell as comment."
      ],
      "metadata": {
        "id": "ELCdCvVKXtTx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "from warnings import filterwarnings\n",
        "\n",
        "filterwarnings(\"ignore\")\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import MACCSkeys, Draw\n",
        "from rdkit.Chem.AllChem import GetMorganFingerprintAsBitVect\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import metrics\n",
        "import seaborn as sns\n",
        "\n",
        "# Neural network specific libraries\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "# all important packages and libraries that are needed later are imported."
      ],
      "metadata": {
        "id": "f_2WBKBE6uXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "HERE = Path(_dh[-1])\n",
        "DATA = HERE / \"data\"\n",
        "\n",
        "# The path is set to the notebook."
      ],
      "metadata": {
        "id": "-6CTu5F96yWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df = pd.read_csv(DATA / \"CHEMBL25_activities_EGFR.csv\", index_col=0)\n",
        "df = df.reset_index(drop=True)\n",
        "\n",
        "# The path of the datafile is given and the data thus loaded to the notebook creating a dataframe (df)."
      ],
      "metadata": {
        "id": "Cdo0rEG6601h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"Shape of dataframe : \", df.shape)\n",
        "df.info()\n",
        "\n",
        "# The shape of the dataframe is shown and with the info() function and the dimension and the dtype of each column is shown."
      ],
      "metadata": {
        "id": "NRkz-UnE621S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df.head()\n",
        "\n",
        "# With the head function the first five rows of the dataframe are shown."
      ],
      "metadata": {
        "id": "YmGI6Yv165_C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "chembl_df = df[[\"canonical_smiles\", \"pIC50\"]]\n",
        "chembl_df.head()\n",
        "\n",
        "# A new dataframe is created (chembl_df) with only the necessary columns for the machine learning model. The first five rows are again shown with the head() function."
      ],
      "metadata": {
        "id": "97fJSKW06-g5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def smiles_to_fp(smiles, method=\"maccs\", n_bits=2048):\n",
        "    \"\"\"\n",
        "    Encode a molecule from a SMILES string into a fingerprint.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    smiles : str\n",
        "        The SMILES string defining the molecule.\n",
        "\n",
        "    method : str\n",
        "        The type of fingerprint to use. Default is MACCS keys.\n",
        "\n",
        "    n_bits : int\n",
        "        The length of the fingerprint.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    array\n",
        "        The fingerprint array.\n",
        "    \"\"\"\n",
        "\n",
        "    # Convert smiles to RDKit mol object\n",
        "    mol = Chem.MolFromSmiles(smiles)\n",
        "\n",
        "    if method == \"maccs\":\n",
        "        return np.array(MACCSkeys.GenMACCSKeys(mol))\n",
        "    if method == \"morgan2\":\n",
        "        return np.array(GetMorganFingerprintAsBitVect(mol, 2, nBits=n_bits))\n",
        "    if method == \"morgan3\":\n",
        "        return np.array(GetMorganFingerprintAsBitVect(mol, 3, nBits=n_bits))\n",
        "    else:\n",
        "        print(f\"Warning: Wrong method specified: {method}.\" \" Default will be used instead.\")\n",
        "        return np.array(MACCSkeys.GenMACCSKeys(mol))\n",
        "\n",
        "# The smiles strings are converted to numerical data. "
      ],
      "metadata": {
        "id": "9R0ZJ2_u6_nY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chembl_df[\"fingerprints_df\"] = chembl_df[\"canonical_smiles\"].apply(smiles_to_fp)\n",
        "\n",
        "# Look at head\n",
        "print(\"Shape of dataframe:\", chembl_df.shape)\n",
        "chembl_df.head(3)\n",
        "# NBVAL_CHECK_OUTPUT\n",
        "\n",
        "# The smiles strings are now converted to MACCS (Molecular Access System) fingerprints."
      ],
      "metadata": {
        "id": "rulyP4HV7CTY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(\n",
        "    chembl_df[\"fingerprints_df\"], chembl_df[[\"pIC50\"]], test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Print the shape of training and testing data\n",
        "print(\"Shape of training data:\", x_train.shape)\n",
        "print(\"Shape of test data:\", x_test.shape)\n",
        "# NBVAL_CHECK_OUTPUT\n",
        "\n",
        "# With the train_test_split() function the dataset is split to a training data, which contains the first 70% of the data, and the test set containing the rest of data. x values are the features (vit vectors) and y the target values."
      ],
      "metadata": {
        "id": "J9hRBDVA7EMY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def neural_network_model(hidden1, hidden2):\n",
        "    \"\"\"\n",
        "    Creating a neural network from two hidden layers\n",
        "    using ReLU as activation function in the two hidden layers\n",
        "    and a linear activation in the output layer.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    hidden1 : int\n",
        "        Number of neurons in first hidden layer.\n",
        "\n",
        "    hidden2: int\n",
        "        Number of neurons in second hidden layer.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    model\n",
        "        Fully connected neural network model with two hidden layers.\n",
        "    \"\"\"\n",
        "\n",
        "    model = Sequential()\n",
        "    # First hidden layer\n",
        "    model.add(Dense(hidden1, activation=\"relu\", name=\"layer1\"))\n",
        "    # Second hidden layer\n",
        "    model.add(Dense(hidden2, activation=\"relu\", name=\"layer2\"))\n",
        "    # Output layer\n",
        "    model.add(Dense(1, activation=\"linear\", name=\"layer3\"))\n",
        "\n",
        "    # Compile model\n",
        "    model.compile(loss=\"mean_squared_error\", optimizer=\"adam\", metrics=[\"mse\", \"mae\"])\n",
        "    return model\n",
        "\n",
        "# The neural network model is defined with two hidden layers. Then the model was compiled with mean sqaured error as loss argument."
      ],
      "metadata": {
        "id": "eHpJPQ4o7Gcw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_sizes = [16, 32, 64]\n",
        "nb_epoch = 50\n",
        "layer1_size = 64\n",
        "layer2_size = 32\n",
        "\n",
        "# different batch sizes are tested."
      ],
      "metadata": {
        "id": "6HtKv3B_7IhI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "fig = plt.figure(figsize=(12, 6))\n",
        "sns.set(color_codes=True)\n",
        "for index, batch in enumerate(batch_sizes):\n",
        "    fig.add_subplot(1, len(batch_sizes), index + 1)\n",
        "    model = neural_network_model(layer1_size, layer2_size)\n",
        "\n",
        "    # Fit model on x_train, y_train data\n",
        "    history = model.fit(\n",
        "        np.array(list((x_train))).astype(float),\n",
        "        y_train.values,\n",
        "        batch_size=batch,\n",
        "        validation_data=(np.array(list((x_test))).astype(float), y_test.values),\n",
        "        verbose=0,\n",
        "        epochs=nb_epoch,\n",
        "    )\n",
        "    plt.plot(history.history[\"loss\"], label=\"train\")\n",
        "    plt.plot(history.history[\"val_loss\"], label=\"test\")\n",
        "    plt.legend([\"train\", \"test\"], loc=\"upper right\")\n",
        "    plt.ylabel(\"loss\")\n",
        "    plt.xlabel(\"epoch\")\n",
        "    plt.ylim((0, 15))\n",
        "    plt.title(\n",
        "        f\"test loss = {history.history['val_loss'][nb_epoch-1]:.2f}, \" f\"batch size = {batch}\"\n",
        "    )\n",
        "plt.show()\n",
        "\n",
        "# The loss functions of the batch sizes are plotted to see which size gives the best performance."
      ],
      "metadata": {
        "id": "QaRPuRnJ7K2Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "filepath = DATA / \"best_weights.hdf5\"\n",
        "checkpoint = ModelCheckpoint(\n",
        "    str(filepath),\n",
        "    monitor=\"loss\",\n",
        "    verbose=0,\n",
        "    save_best_only=True,\n",
        "    mode=\"min\",\n",
        "    save_weights_only=True,\n",
        ")\n",
        "callbacks_list = [checkpoint]\n",
        "\n",
        "# Fit the model\n",
        "model.fit(\n",
        "    np.array(list((x_train))).astype(float),\n",
        "    y_train.values,\n",
        "    epochs=nb_epoch,\n",
        "    batch_size=16,\n",
        "    callbacks=callbacks_list,\n",
        "    verbose=0,\n",
        ")\n",
        "\n",
        "# The trained model with the best performance is saved. "
      ],
      "metadata": {
        "id": "KTSsEfha7MUx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(f\"Evaluate the model on the test data\")\n",
        "scores = model.evaluate(np.array(list((x_test))), y_test.values, verbose=0)\n",
        "print(f\" loss: {scores[0]:.2f}\")\n",
        "print(f\" mse (same as loss): {scores[1]:.2f}\")\n",
        "print(f\" mae: {scores[2]:.2f}\")\n",
        "\n",
        "# The model's performance is checked and evaluated and the loss is shown."
      ],
      "metadata": {
        "id": "IxH5uDCn7OQR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "y_pred = model.predict(np.array(list((x_test))))\n",
        "\n",
        "# Print 5 first pIC50 predicted values\n",
        "first_5_prediction = [print(f\"{value[0]:.2f}\") for value in y_pred[0:5]]\n",
        "\n",
        "# Some values of the test data are predicted. "
      ],
      "metadata": {
        "id": "Qin0Dtij7P7p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "limits = 0, 15\n",
        "fig, ax = plt.subplots()\n",
        "ax.scatter(y_pred, y_test, marker=\".\")\n",
        "lin = np.linspace(*limits, 100)\n",
        "ax.plot(lin, lin)\n",
        "ax.set_aspect(\"equal\", adjustable=\"box\")\n",
        "ax.set_xlabel(\"Predicted values\")\n",
        "ax.set_ylabel(\"True values\")\n",
        "ax.set_title(\"Scatter plot: pIC50 values\")\n",
        "ax.set_xlim(limits)\n",
        "ax.set_ylim(limits)\n",
        "plt.show()\n",
        "\n",
        "# The predicted values are plotted against the real values from the test set in a scatter plot to see if the predictions were correct."
      ],
      "metadata": {
        "id": "ysUnqDVd7SbZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "external_data = pd.read_csv(DATA / \"test.csv\", index_col=0)\n",
        "external_data = external_data.reset_index(drop=True)\n",
        "external_data.head()\n",
        "# NBVAL_CHECK_OUTPUT\n",
        "\n",
        "# The test.csv file contains unlabeled compounds, that are used to train the model to predict the values. The file is loaded to the notebook and saved as dataframe under external_data."
      ],
      "metadata": {
        "id": "ACgf-GIi7VTJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "external_data[\"fingerprints_df\"] = external_data[\"canonical_smiles\"].apply(smiles_to_fp)\n",
        "\n",
        "# Look at head\n",
        "print(\"Shape of dataframe : \", external_data.shape)\n",
        "external_data.head(3)\n",
        "# NBVAL_CHECK_OUTPUT\n",
        "\n",
        "# The smiles strings are again converted into MACCS fingerprints and the first three rows are shown by the head() function."
      ],
      "metadata": {
        "id": "6va7mLcX7XEy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model = load_model(DATA / \"ANN_model.hdf5\", compile=False)\n",
        "\n",
        "# The model loaded and saved."
      ],
      "metadata": {
        "id": "nKjBUFSm7ZgI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "predictions = model.predict(\n",
        "    np.array(list((external_data[\"fingerprints_df\"]))).astype(float), callbacks=callbacks_list\n",
        ")\n",
        "\n",
        "predicted_pIC50 = pd.DataFrame(predictions, columns=[\"predicted_pIC50\"])\n",
        "predicted_pIC50_df = external_data.join(predicted_pIC50)\n",
        "\n",
        "predicted_pIC50_df.head(3)\n",
        "\n",
        "# The pIC50 values are predicted from the MACCS fingerprints."
      ],
      "metadata": {
        "id": "6Et8ARIf7bEJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_pIC50_df.to_csv(DATA / \"predicted_pIC50_df.csv\")\n",
        "\n",
        "# The predicted values are saved in a csv file. "
      ],
      "metadata": {
        "id": "SOdo5LQbhoq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "predicted_pIC50_df = pd.read_csv(DATA / \"predicted_pIC50_df.csv\", index_col=0)\n",
        "top3_drug = predicted_pIC50_df.nlargest(3, \"predicted_pIC50\")\n",
        "top3_drug\n",
        "\n",
        "# The smiles with the highest predicted values are shown."
      ],
      "metadata": {
        "id": "F0CuNbg77cSx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "highest_pIC50 = predicted_pIC50_df[\"canonical_smiles\"][top3_drug.index]\n",
        "\n",
        "mols_EGFR = [Chem.MolFromSmiles(smile) for smile in highest_pIC50]\n",
        "pIC50_EGFR = top3_drug[\"predicted_pIC50\"].tolist()\n",
        "pIC50_values = [(f\"pIC50 value: {value:.2f}\") for value in pIC50_EGFR]\n",
        "\n",
        "Draw.MolsToGridImage(mols_EGFR, molsPerRow=3, subImgSize=(450, 300), legends=pIC50_values)\n",
        "\n",
        "# The structures of the molecules with the highest predicted values are shown."
      ],
      "metadata": {
        "id": "Q5PevUqM7ewp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 5: Ubelix questions**"
      ],
      "metadata": {
        "id": "foinuPIQkGgc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is Ubelix?"
      ],
      "metadata": {
        "id": "FLcwg0AU2D4r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ubelix is an HPC (High Performance Computing) cluster and the abbreviation of University of Bern Linux cluster. It runs about 320 compute nodes. For problems with high computing power, Ubelix can be used to spread the problem over several computers and thus greatly reduce the computing time."
      ],
      "metadata": {
        "id": "XWrpk2Zy2HgF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. How do you gain access?"
      ],
      "metadata": {
        "id": "dVqaYSgf2MMQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First the Campus Account must be activated for Ubelix by sending a request to the service portal of the University of Bern. After activation, you can get access by typing one of the following commands: ssh <username>@submit[01-04].unibe.ch and then the password of the CA while being connected to the network of University of Bern or VPN."
      ],
      "metadata": {
        "id": "E3zajNe52OtW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. How do you submit a job?"
      ],
      "metadata": {
        "id": "_HXwbNUo2OxU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A job can be submitted by the input sbatch jobs.sh and then !/bin/bash. There are other allocations (nodes, cores) than sbash like salloc or srun, but sbash is the most common. Then the name of the job, the runtime, number of tasks, nodes etc. can be defined and the job submitted."
      ],
      "metadata": {
        "id": "IgSYg0kG2O25"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Who can have access?"
      ],
      "metadata": {
        "id": "d3idTpso2TAN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Everyone of the University of Bern with a valid Campus account can get access to Ubelix."
      ],
      "metadata": {
        "id": "Luj_W2OD2XKh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. What resources are available there?"
      ],
      "metadata": {
        "id": "KPF5CSqI2XX9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1TB of disk space, 20TB of temporary scratch space for raw data/results and access to CPUs and GPUs for computation within limits."
      ],
      "metadata": {
        "id": "8TUlB5mx2b-7"
      }
    }
  ]
}